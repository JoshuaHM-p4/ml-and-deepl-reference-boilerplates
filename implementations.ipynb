{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning \n",
    "1. Import Libraries\n",
    "2. Import and Read the Data \n",
    "3. Split the Data Sets into: \n",
    "\ta. Training Set \n",
    "\tb. Cross-validation/Validation/Dev Set \n",
    "\tc. Test Set\n",
    "4. Modify or Feature Engineer the data set \n",
    "    - Handle missing data, create new features, encode categorical variables, etc.\n",
    "5. (If Needed) Set feature scaling to the three data sets \n",
    "6. Create the Model \n",
    "\t- linear output for \n",
    "7. Train the Model \n",
    "\t- Set the loss function (SparseCategoricalCrossEntropy, BinaryCrossEntropy, etc.)\n",
    "\t- Set the Use adam algorithm\n",
    "8. Evaluate the Model\n",
    "    - Assess model performance using metrics (e.g., accuracy, F1-score, RMSE) on the test set.\n",
    "    - Generate confusion matrices, ROC curves, etc., as needed.\n",
    "9. Hyperparameter Tuning\n",
    "    - Optimize hyperparameters (learning rate, regularization strength, etc.) \n",
    "    - Using grid search, random search, or manual tuning on the validation set.\n",
    "10. Cross-Validation (if needed)\n",
    "    - Use k-fold cross-validation to further validate the robustness of the model.\n",
    "11. Model Deployment (optional)\n",
    "    - Save the model, implement it in a production environment, and monitor its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Dot\n",
    "from tensorflow.keras.activations import linear, relu, softmax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.losses import MeanSquaredError, BinaryCrossentropy, SparseCategoricalCrossentropy, CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.linalg import norm, l2_normalize\n",
    "\n",
    "# Data and Evaluation Tools\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV,\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "tf.autograph.set_verbosity(0)\n",
    "np.set_printoptions(precision = 2)\n",
    "## IMPORT LIBRARIES ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "1. Import Libraries\n",
    "2. Import and Read the Data \n",
    "3. Split the Data Sets into: \n",
    "\ta. Training Set \n",
    "\tb. Test Set\n",
    "4. Modify or Feature Engineer the data set \n",
    "    - Handle missing data, create new features, encode categorical variables, etc.\n",
    "5. (If Needed) Set feature scaling to the three data sets \n",
    "6. Create the Model \n",
    "\t- linear output for \n",
    "7. Train the Model \n",
    "\t- Set the loss function (SparseCategoricalCrossEntropy, BinaryCrossEntropy, etc.)\n",
    "\t- Set the Use adam algorithm\n",
    "8. Hyperparameter Tuning\n",
    "    - Optimize hyperparameters (e.g., max depth, min samples split) using grid search, random search, or cross-validation.\n",
    "9. Pruning (optional)\n",
    "    - Apply pruning techniques to avoid overfitting and improve generalization.\n",
    "10. Model Interpretation\n",
    "    - Interpret the decision tree model by visualizing the tree or using feature importance.\n",
    "11. Model Deployment (optional)\n",
    "    - Save the model, implement it in a production environment, and monitor its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import xgboost\n",
    "\n",
    "# Data and Evaluation Tools\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Randomized Search CV\n",
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typical Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for duplicates\n",
    "duplicate_rows = data[data.duplicated()]\n",
    "\n",
    "# Dropping duplicates\n",
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "\n",
    "# Dropping columns\n",
    "df = df.drop(columns = ['column1', 'column2', 'column3'])\n",
    "\n",
    "# Dropping rows with missing values\n",
    "df = df.dropna(subset = ['column1', 'column2', 'column3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying garbage values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.select_dtypes(include='object').columns:\n",
    "    print(df[i].value_counts())\n",
    "    print(\"***\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting columns for features and target\n",
    "X = df.iloc[:, START_COL:END_COL]\n",
    "y = df.iloc[:, TARGET_COL]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Impute missing values\n",
    "imp_mean = SimpleImputer(strategy = 'mean') # Mean for numerical values\n",
    "X = imp_mean.fit_transform(X)\n",
    "\n",
    "imp_median = SimpleImputer(strategy = 'median') # Median for numerical values\n",
    "X = imp_median.fit_transform(X)\n",
    "\n",
    "imp_mode = SimpleImputer(strategy = 'most_frequent') # Most frequent for categorical values\n",
    "X = imp_mode.fit_transform(X)\n",
    "\n",
    "imp_constant_cat = SimpleImputer(strategy = 'constant', fill_value = 0) # Constant for categorical and/or numerical values\n",
    "X = imp_constant_cat.fit_transform(X)\n",
    "\n",
    "imp_mean_marked = SimpleImputer(strategy = 'mean', add_indicator = True) # Add indicator for missing values\n",
    "X = imp_mean_marked.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas Implementation\n",
    "cat_variables = ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope'] # Columns with categorical variables\n",
    "df = pd.get_dummies(data = df, prefix = cat_variables, columns = cat_variables)\n",
    "\n",
    "######################################################################################\n",
    "\n",
    "# SciKit Learn Implementation\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False) # Sparse output is False for a DataFrame\n",
    "df_encoded = ohe.fit_transform(df[cat_variables])\n",
    "df_encoded = pd.DataFrame(df_encoded, columns = ohe.get_feature_names(cat_variables))\n",
    "df = pd.concat([df, df_encoded], axis=1)\n",
    "df = df.drop(columns = cat_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinal Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas Implementation\n",
    "df['column'] = df['column'].astype('category').cat.codes\n",
    "\n",
    "# for loop\n",
    "for categ in cat_variables:\n",
    "    df[categ] = df[categ].astype('category').cat.codes\n",
    "\n",
    "######################################################################################\n",
    "\n",
    "# Scikit Learn Implementation\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ode = OrdinalEncoder()\n",
    "df['column'] = ode.fit_transform(df['column'].values.reshape(-1, 1))\n",
    "\n",
    "# for loop\n",
    "for categ in cat_variables:\n",
    "    df[categ] = ode.fit_transform(df[categ].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "df = pd.read_csv('data.csv') # Load data\n",
    "\n",
    "# Column transformer for imputing missing values one hot encoding data types (categorical and numerical)\n",
    "ct = make_column_transformer(\n",
    "    (ode, [\"Sport-type\"]), # Ordinal encode 'Sport-type' column\n",
    "    (ohe, [\"gender\"]), # One-hot encode 'gender' column\n",
    "    remainder = 'passthrough' # Pass through the remaining columns\n",
    ")\n",
    "\n",
    "ct = make_column_transformer(\n",
    "    (imp_constant_cat, ['Name']), # Impute missing values in 'Name' column with a constant\n",
    "    (imp_mean, [\"farthest_run_mi\"]), # Impute missing values in 'farthest_run_mi' column with the mean\n",
    "    remainder = 'passthrough' # Pass through the remaining columns\n",
    ")\n",
    "\n",
    "ct.set_output(transform='pandas')\n",
    "df = ct.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Normally, Pipelines are used to allow chain multiple transformations together for a single column. Further shown later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to remove outliers based on the IQR method\n",
    "def remove_outliers(df, feature):\n",
    "    \"\"\"\n",
    "    Remove outliers from a specified feature in the DataFrame using the IQR method.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame from which to remove outliers.\n",
    "    feature (str): The name of the column (feature) to check for outliers.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with outliers removed for the specified feature.\n",
    "    \"\"\"\n",
    "    # Calculate the first quartile (Q1) and third quartile (Q3) of the feature\n",
    "    Q1 = df[feature].quantile(0.25)\n",
    "    Q3 = df[feature].quantile(0.75)\n",
    "\n",
    "    # Calculate the Interquartile Range (IQR)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Define the outlier bounds\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Filter the DataFrame to exclude outliers\n",
    "    df_cleaned = df[(df[feature] >= lower_bound) & (df[feature] <= upper_bound)]\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "# List of features to check for outliers\n",
    "features = ['Age', 'Weight', 'Height', 'BMI']\n",
    "\n",
    "# Apply the outlier removal function to each feature\n",
    "for feature in features:\n",
    "    data = remove_outliers(data, feature)\n",
    "\n",
    "# Display the cleaned DataFrame shape\n",
    "print(\"Shape of DataFrame after outlier removal:\", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Datasets \n",
    "into (1) Training Set and (2) Test Set using Scikit-learn train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get 60% of the dataset as the training set. Put the remaining 40% in temporary variables: x_ and y_.\n",
    "x_train, x_, y_train, y_ = train_test_split(x, y, test_size=0.40, random_state=1)\n",
    "\n",
    "# Split the 40% subset above into two: one half for cross validation and the other for the test set\n",
    "x_cv, x_test, y_cv, y_test = train_test_split(x_, y_, test_size=0.50, random_state=1)\n",
    "\n",
    "# Delete temporary variables\n",
    "del x_, y_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# MinMaxScaler for target variables\n",
    "scaler = MinMaxScaler((-1, 1)) # Feature range parameter: from -1 to 1, typically used for y_train\n",
    "scaler.fit(y_train.reshape(-1, 1))\n",
    "y_train = scaler.transform(y_train.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# X_train = df.iloc[:, 1:].to_numpy()\n",
    "# labels = df.columns.to_numpy()\n",
    "\n",
    "# Standard Scaler for features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "# X_train = pd.DataFrame(X_train, columns = labels) # If converted back to a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preoprocessing import StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [\"Social_media_followers\"] # numerical columns\n",
    "cat_cols = [\"Genre\"] # categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for numerical columns\n",
    "num_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('impute', SimpleImputer(strategy = 'mean')),\n",
    "        ('scale', StandardScaler())\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Pipeline for categorical columns\n",
    "cat_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('impute', SimpleImputer(strategy = 'most_frequent')),\n",
    "        ('encode', OneHotEncoder(handle_unknown = 'ignore', ))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column Transformer with Pipelines\n",
    "col_transformer = ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('num_pipeline', num_pipeline, num_cols), # Apply the num_pipeline to the numerical columns\n",
    "        ('cat_pipeline', cat_pipeline, cat_cols) # Apply the cat_pipeline to the categorical columns\n",
    "    ],\n",
    "    remainder = 'drop', # Drop columns not specified\n",
    "    n_jobs = -1         # Use all processors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.compose import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier()\n",
    "pipe_final = make_pipeline(col_transformer, dtc) # Combine the column transformer and the model\n",
    "pipe_final.fit(X_train, y_train)\n",
    "pipe_final.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Pipeline\n",
    "import joblib\n",
    "joblib.dump(pipe_final, \"pipe.jolib\")\n",
    "\n",
    "# Loading Pipeline\n",
    "pipe_final2 = joblib.load('pipe.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter Tuning\n",
    "\n",
    "Resource for parameters can be always found through docs: \n",
    "<ul>\n",
    "<li><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">Random Forest</a></li>\n",
    "<li><a href=\"https://xgboost.readthedocs.io/en/stable/parameter.html\">XGBoost</a></li>\n",
    "<!-- <li><a href=\"\"></a></li> -->\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Grid Search Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"n_estimators\": ['entropy', 'gain'],\n",
    "    \"min_samples_split\": [5, 10, 15],\n",
    "    \"min_samples_leaf\": [1,2,4],\n",
    "    \"max_depth\": [10, 20, 30]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestClassifier(),\n",
    "    param_grid,\n",
    "    cv = 5,\n",
    "    scoring = 'accuracy' # for regression task use \"neg_mean_squared_error\" or look at DOCS: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train) # make sure y_train is a 1D array by using y_train.values.ravel() or y_train.reshape(-1,)\n",
    "# grid_search.best_score_ and grid_search.best_params_ will give you the best score and the best parameters respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grid_search' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241m.\u001b[39mbest_score_ \u001b[38;5;66;03m# Best score\u001b[39;00m\n\u001b[0;32m      2\u001b[0m grid_search\u001b[38;5;241m.\u001b[39mbest_params_ \u001b[38;5;66;03m# Best parameters\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'grid_search' is not defined"
     ]
    }
   ],
   "source": [
    "grid_search.best_score_ # Best score (according to the scoring parameter, if \"neg_mean_squared_error\" is used, negate the value)\n",
    "grid_search.best_estimator_ # Best estimator\n",
    "grid_search.best_params_ # Best parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Randomized Search Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_param_grid = [{\n",
    "    \"n_estimators\": ['entropy', 'gain'],\n",
    "    \"min_samples_split\": [5, 10, 15],\n",
    "    \"min_samples_leaf\": [1,2,4],\n",
    "    \"max_depth\": [10, 20, 30]\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    RandomForestClassifier(),\n",
    "    random_param_grid,\n",
    "    n_iter = 10,\n",
    "    cv = 5,\n",
    "    scoring = 'accuracy'\n",
    ")\n",
    "\n",
    "# random_search.best_score_ and random_search.best_params_ will give you the best score and the best parameters respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Hyperparameter Tuning for Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Using Scikeras wrapper for implementing a neural network model built from keras for Scikit-learn to be used for cross-validation search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikeras # !pip install scikeras for Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model build function for KerasRegressor\n",
    "def build_regressor_model(n_hidden, n_neurons, learning_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_neurons, activation = 'relu', input_shape = (X_train.shape[0],)))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(Dense(n_neurons, activation = 'relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer = Adam(learning_rate = learning_rate), loss = 'mean_squared_error')\n",
    "    return model\n",
    "\n",
    "def build_binary_classifier(n_hidden, n_neurons, learning_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_neurons, activation = 'relu', input_shape = (X_train.shape[0],)))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(Dense(n_neurons))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    model.compile(optimizer = Adam(learning_rate = learning_rate), loss = 'binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "def build_multiclass_classifier(n_hidden, n_neurons, learning_rate, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_neurons, activation = 'relu', input_shape = (X_train.shape[0],)))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(Dense(n_neurons, activation = 'relu'))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.compile(optimizer = Adam(learning_rate = learning_rate), loss = 'categorical_crossentropy')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
